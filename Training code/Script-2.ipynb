{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89890bde-b9cd-476d-8f72-ed471a0adb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers datasets evaluate accelerate sentencepiece tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e3e30a-9fce-4f6d-b938-0d52eeb517f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 610767/610767 [00:02<00:00, 232329.46 examples/s]\n",
      "Generating dev split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 261758/261758 [00:01<00:00, 245625.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Jinyan1/COLING_2025_MGT_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a91756-3391-4bd3-b2d5-048e85642b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41e5516f-58f9-4750-92d1-f7e6d8c67f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = ds['train'].to_pandas()[['text','label']].sample(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "150ae291-b01c-4e2b-b072-c6f2e04b9804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'dev' split to a Pandas DataFrame and rename the columns\n",
    "df = ds['dev'].to_pandas()[['text', 'label']].rename(columns={'labels': 'label'})\n",
    "\n",
    "# Separate dataframes for label 0 and label 1\n",
    "df_label_0 = df[df['label'] == 0]\n",
    "df_label_1 = df[df['label'] == 1]\n",
    "# Sample 1000 rows from each dataframe\n",
    "sample_label_0 = df_label_0.sample(n=1000, random_state=42)  # Set random_state for reproducibility\n",
    "sample_label_1 = df_label_1.sample(n=1000, random_state=42)  # Set random_state for reproducibility\n",
    "\n",
    "# Concatenate the samples\n",
    "test_df = pd.concat([sample_label_0, sample_label_1], axis=0).sample(frac=1, random_state=42) #frac=1 for shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa4f2c25-f3a1-4e1b-addb-af6825e5f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train_df (1).csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a13e5eb7-28e7-4498-bf4b-9d756c385de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1874\n",
      "0    1126\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train['label'].value_counts())\n",
    "#print(test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e54fd4f-f545-4783-8364-b4e3afbff53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('test_df (1).csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767824e8-cd6d-476d-9366-fdacec612e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading data...\n",
      "INFO:__main__:Preprocessing data...\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:00<00:00, 5064.60 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:00<00:00, 4935.39 examples/s]\n",
      "INFO:__main__:Training model...\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at vicgalle/xlm-roberta-large-xnli-anli and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 1024]) in the checkpoint and torch.Size([2, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3337/2826770542.py:162: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='711' max='710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [710/710 04:06, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.569084</td>\n",
       "      <td>0.682667</td>\n",
       "      <td>0.680425</td>\n",
       "      <td>0.682667</td>\n",
       "      <td>0.681424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.752342</td>\n",
       "      <td>0.705333</td>\n",
       "      <td>0.748479</td>\n",
       "      <td>0.705333</td>\n",
       "      <td>0.649923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.199776</td>\n",
       "      <td>0.705333</td>\n",
       "      <td>0.754552</td>\n",
       "      <td>0.705333</td>\n",
       "      <td>0.647777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.075016</td>\n",
       "      <td>0.741333</td>\n",
       "      <td>0.737330</td>\n",
       "      <td>0.741333</td>\n",
       "      <td>0.729296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.927470</td>\n",
       "      <td>0.737333</td>\n",
       "      <td>0.754391</td>\n",
       "      <td>0.737333</td>\n",
       "      <td>0.707826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.231930</td>\n",
       "      <td>0.722667</td>\n",
       "      <td>0.749519</td>\n",
       "      <td>0.722667</td>\n",
       "      <td>0.682757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.130319</td>\n",
       "      <td>0.718667</td>\n",
       "      <td>0.740394</td>\n",
       "      <td>0.718667</td>\n",
       "      <td>0.679422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.226100</td>\n",
       "      <td>2.266162</td>\n",
       "      <td>0.721333</td>\n",
       "      <td>0.732396</td>\n",
       "      <td>0.721333</td>\n",
       "      <td>0.689319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.226100</td>\n",
       "      <td>2.284920</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.742575</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.706949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import DebertaV2Tokenizer, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "import logging\n",
    "import os\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CFG:\n",
    "    \"\"\"Configuration class for training parameters\"\"\"\n",
    "    train_path = 'train_df (1).csv'\n",
    "    test_path = 'test_df (1).csv'\n",
    "    model_name = \"vicgalle/xlm-roberta-large-xnli-anli\"\n",
    "    epochs = 10\n",
    "    learning_rate = 0.00003\n",
    "    batch_size = 32\n",
    "    max_length = 128\n",
    "    test_size = 0.25\n",
    "    output_dir = \"./model\"\n",
    "    checkpoint_dir = \"./checkpoints\"\n",
    "    random_seed = 42\n",
    "\n",
    "class TextClassificationTrainer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            CFG.model_name,\n",
    "            ignore_mismatched_sizes=True\n",
    "            #add_prefix_space=True  # This can help with better tokenization\n",
    "        )\n",
    "        self.setup_directories()\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_directories():\n",
    "        \"\"\"Create necessary directories for model and checkpoint saving\"\"\"\n",
    "        os.makedirs(CFG.output_dir, exist_ok=True)\n",
    "        os.makedirs(CFG.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Load training and test data from CSV files\"\"\"\n",
    "        try:\n",
    "            train = pd.read_csv(CFG.train_path)\n",
    "            test = pd.read_csv(CFG.test_path)\n",
    "\n",
    "            # Validate required columns\n",
    "            required_columns = ['text', 'label']\n",
    "            for df, name in [(train, 'train'), (test, 'test')]:\n",
    "                missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "                if missing_cols:\n",
    "                    raise ValueError(f\"{name} dataset missing required columns: {missing_cols}\")\n",
    "\n",
    "            return train, test\n",
    "        except FileNotFoundError as e:\n",
    "            raise Exception(f\"Error loading data: {e}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Unexpected error while loading data: {e}\")\n",
    "\n",
    "    def preprocess_function(self, examples):\n",
    "        \"\"\"Tokenize text data\"\"\"\n",
    "        return self.tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=CFG.max_length,\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "    def preprocess_data(self, train: pd.DataFrame, test: pd.DataFrame) -> Tuple[DatasetDict, Dataset]:\n",
    "        \"\"\"Convert DataFrames to Datasets and preprocess\"\"\"\n",
    "        try:\n",
    "            # Convert to Dataset format\n",
    "            ds = Dataset.from_pandas(train)\n",
    "            ds_test = Dataset.from_pandas(test)\n",
    "\n",
    "            # Tokenize datasets\n",
    "            tok_ds = ds.map(self.preprocess_function, batched=True)\n",
    "            dds = tok_ds.train_test_split(test_size=CFG.test_size, seed=CFG.random_seed)\n",
    "            eval_dataset = ds_test.map(self.preprocess_function, batched=True)\n",
    "\n",
    "            return dds, eval_dataset\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in data preprocessing: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_metrics(eval_pred) -> Dict[str, float]:\n",
    "        \"\"\"Compute evaluation metrics\"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "        metrics = {}\n",
    "        \n",
    "        # Accuracy\n",
    "        accuracy_metric = evaluate.load(\"accuracy\")\n",
    "        accuracy_result = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "        metrics[\"accuracy\"] = accuracy_result[\"accuracy\"]\n",
    "    \n",
    "        # Precision\n",
    "        precision_metric = evaluate.load(\"precision\")\n",
    "        precision_result = precision_metric.compute(\n",
    "            predictions=predictions,\n",
    "            references=labels,\n",
    "            average=\"weighted\",\n",
    "            zero_division=0\n",
    "        )\n",
    "        metrics[\"precision\"] = precision_result[\"precision\"]\n",
    "    \n",
    "        # Recall\n",
    "        recall_metric = evaluate.load(\"recall\")\n",
    "        recall_result = recall_metric.compute(\n",
    "            predictions=predictions,\n",
    "            references=labels,\n",
    "            average=\"weighted\",\n",
    "            zero_division=0\n",
    "        )\n",
    "        metrics[\"recall\"] = recall_result[\"recall\"]\n",
    "    \n",
    "        # F1 Score\n",
    "        f1_metric = evaluate.load(\"f1\")\n",
    "        f1_result = f1_metric.compute(\n",
    "            predictions=predictions,\n",
    "            references=labels,\n",
    "            average=\"weighted\"\n",
    "        )\n",
    "        metrics[\"f1\"] = f1_result[\"f1\"]\n",
    "    \n",
    "        return metrics\n",
    "\n",
    "    def train_model(self, dds: DatasetDict) -> Trainer:\n",
    "        \"\"\"Initialize and train the model\"\"\"\n",
    "        try:\n",
    "            # Initialize model and data collator\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                CFG.model_name,\n",
    "                num_labels=2,ignore_mismatched_sizes=True\n",
    "            )\n",
    "            data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "\n",
    "            # Set up training arguments\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=CFG.output_dir,\n",
    "                learning_rate=CFG.learning_rate,\n",
    "                per_device_train_batch_size=CFG.batch_size,\n",
    "                per_device_eval_batch_size=CFG.batch_size,\n",
    "                num_train_epochs=CFG.epochs,\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                save_strategy=\"epoch\",\n",
    "                save_total_limit=2,\n",
    "                fp16=torch.cuda.is_available(),\n",
    "                logging_dir=f\"{CFG.output_dir}/logs\",\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"f1\",\n",
    "            )\n",
    "\n",
    "            # Initialize trainer\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=dds['train'],\n",
    "                eval_dataset=dds['test'],\n",
    "                tokenizer=self.tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=self.compute_metrics,\n",
    "            )\n",
    "\n",
    "            # Train the model\n",
    "            trainer.train()\n",
    "            return trainer\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in model training: {e}\")\n",
    "\n",
    "    def get_predictions(self, trainer: Trainer, eval_dataset: Dataset) -> pd.DataFrame:\n",
    "        \"\"\"Get probability predictions for the evaluation dataset and return as DataFrame\"\"\"\n",
    "        try:\n",
    "            predictions = trainer.predict(eval_dataset)\n",
    "            logits = predictions.predictions\n",
    "            probabilities = softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "\n",
    "            # Create DataFrame with model-specific column names\n",
    "            model_name = CFG.model_name.split('/')[-1]  # Get last part of model name\n",
    "            df_predictions = pd.DataFrame(\n",
    "                probabilities,\n",
    "                columns=[f'p0_{model_name}', f'p1_{model_name}']\n",
    "            )\n",
    "\n",
    "            return df_predictions\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in getting predictions: {e}\")\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up GPU memory\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        # Initialize trainer\n",
    "        text_classifier = TextClassificationTrainer()\n",
    "\n",
    "        # Load and preprocess data\n",
    "        logger.info(\"Loading data...\")\n",
    "        train, test = text_classifier.load_data()\n",
    "\n",
    "        logger.info(\"Preprocessing data...\")\n",
    "        dds, eval_dataset = text_classifier.preprocess_data(train, test)\n",
    "\n",
    "        # Train model\n",
    "        logger.info(\"Training model...\")\n",
    "        trainer = text_classifier.train_model(dds)\n",
    "\n",
    "        # Get predictions as DataFrame\n",
    "        logger.info(\"Getting predictions...\")\n",
    "        df_predictions = text_classifier.get_predictions(trainer, eval_dataset)\n",
    "\n",
    "        # Save predictions DataFrame\n",
    "        output_path = f\"{CFG.output_dir}/predictions_{CFG.model_name.split('/')[-1]}.csv\"\n",
    "        df_predictions.to_csv(output_path, index=False)\n",
    "        logger.info(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "        # Cleanup\n",
    "        text_classifier.cleanup()\n",
    "\n",
    "        return df_predictions\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa7a70-6afe-47e8-8759-e618097927b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "606f1cc0-4fef-43d2-81a0-48165c228deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf49254-2fff-417b-b44f-c45b85cdf7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
