{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89890bde-b9cd-476d-8f72-ed471a0adb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60e3e30a-9fce-4f6d-b938-0d52eeb517f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|██████████| 610767/610767 [00:02<00:00, 227112.42 examples/s]\n",
      "Generating dev split: 100%|██████████| 261758/261758 [00:01<00:00, 245776.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Jinyan1/COLING_2025_MGT_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a91756-3391-4bd3-b2d5-048e85642b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41e5516f-58f9-4750-92d1-f7e6d8c67f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = ds['train'].to_pandas()[['text','label']].sample(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "150ae291-b01c-4e2b-b072-c6f2e04b9804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'dev' split to a Pandas DataFrame and rename the columns\n",
    "df = ds['dev'].to_pandas()[['text', 'label']].rename(columns={'labels': 'label'})\n",
    "\n",
    "# Separate dataframes for label 0 and label 1\n",
    "df_label_0 = df[df['label'] == 0]\n",
    "df_label_1 = df[df['label'] == 1]\n",
    "# Sample 1000 rows from each dataframe\n",
    "sample_label_0 = df_label_0.sample(n=1000, random_state=42)  # Set random_state for reproducibility\n",
    "sample_label_1 = df_label_1.sample(n=1000, random_state=42)  # Set random_state for reproducibility\n",
    "\n",
    "# Concatenate the samples\n",
    "test_df = pd.concat([sample_label_0, sample_label_1], axis=0).sample(frac=1, random_state=42) #frac=1 for shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa4f2c25-f3a1-4e1b-addb-af6825e5f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train_df (1).csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e5eb7-28e7-4498-bf4b-9d756c385de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train['label'].value_counts())\n",
    "#print(test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e54fd4f-f545-4783-8364-b4e3afbff53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('test_df (1).csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "767824e8-cd6d-476d-9366-fdacec612e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error in main execution: \n",
      "DebertaV2Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nDebertaV2Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2095/1392097533.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2095/1392097533.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# Initialize trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mtext_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextClassificationTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# Load and preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2095/1392097533.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTextClassificationTrainer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         self.tokenizer = DebertaV2Tokenizer.from_pretrained(\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0madd_prefix_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m  \u001b[0;31m# This can help with better tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1734\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"_from_config\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m         \u001b[0mrequires_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backends\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1722\u001b[0m     \u001b[0mfailed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchecks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \nDebertaV2Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import DebertaV2Tokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "import logging\n",
    "import os\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CFG:\n",
    "    \"\"\"Configuration class for training parameters\"\"\"\n",
    "    train_path = 'train_df (1).csv'\n",
    "    test_path = 'test_df (1).csv'\n",
    "    model_name = \"microsoft/deberta-v3-large\"\n",
    "    epochs = 10\n",
    "    learning_rate = 0.00003\n",
    "    batch_size = 128\n",
    "    max_length = 128\n",
    "    test_size = 0.25\n",
    "    output_dir = \"./model\"\n",
    "    checkpoint_dir = \"./checkpoints\"\n",
    "    random_seed = 42\n",
    "\n",
    "class TextClassificationTrainer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = DebertaV2Tokenizer.from_pretrained(\n",
    "            CFG.model_name,\n",
    "            add_prefix_space=True  # This can help with better tokenization\n",
    "        )\n",
    "        self.setup_directories()\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_directories():\n",
    "        \"\"\"Create necessary directories for model and checkpoint saving\"\"\"\n",
    "        os.makedirs(CFG.output_dir, exist_ok=True)\n",
    "        os.makedirs(CFG.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Load training and test data from CSV files\"\"\"\n",
    "        try:\n",
    "            train = pd.read_csv(CFG.train_path)\n",
    "            test = pd.read_csv(CFG.test_path)\n",
    "\n",
    "            # Validate required columns\n",
    "            required_columns = ['text', 'label']\n",
    "            for df, name in [(train, 'train'), (test, 'test')]:\n",
    "                missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "                if missing_cols:\n",
    "                    raise ValueError(f\"{name} dataset missing required columns: {missing_cols}\")\n",
    "\n",
    "            return train, test\n",
    "        except FileNotFoundError as e:\n",
    "            raise Exception(f\"Error loading data: {e}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Unexpected error while loading data: {e}\")\n",
    "\n",
    "    def preprocess_function(self, examples):\n",
    "        \"\"\"Tokenize text data\"\"\"\n",
    "        return self.tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=CFG.max_length,\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "    def preprocess_data(self, train: pd.DataFrame, test: pd.DataFrame) -> Tuple[DatasetDict, Dataset]:\n",
    "        \"\"\"Convert DataFrames to Datasets and preprocess\"\"\"\n",
    "        try:\n",
    "            # Convert to Dataset format\n",
    "            ds = Dataset.from_pandas(train)\n",
    "            ds_test = Dataset.from_pandas(test)\n",
    "\n",
    "            # Tokenize datasets\n",
    "            tok_ds = ds.map(self.preprocess_function, batched=True)\n",
    "            dds = tok_ds.train_test_split(test_size=CFG.test_size, seed=CFG.random_seed)\n",
    "            eval_dataset = ds_test.map(self.preprocess_function, batched=True)\n",
    "\n",
    "            return dds, eval_dataset\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in data preprocessing: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_metrics(eval_pred) -> Dict[str, float]:\n",
    "        \"\"\"Compute evaluation metrics\"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "        metrics = {}\n",
    "        \n",
    "        # Accuracy\n",
    "        accuracy_metric = evaluate.load(\"accuracy\")\n",
    "        accuracy_result = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "        metrics[\"accuracy\"] = accuracy_result[\"accuracy\"]\n",
    "    \n",
    "        # Precision\n",
    "        precision_metric = evaluate.load(\"precision\")\n",
    "        precision_result = precision_metric.compute(\n",
    "            predictions=predictions,\n",
    "            references=labels,\n",
    "            average=\"weighted\",\n",
    "            zero_division=0\n",
    "        )\n",
    "        metrics[\"precision\"] = precision_result[\"precision\"]\n",
    "    \n",
    "        # Recall\n",
    "        recall_metric = evaluate.load(\"recall\")\n",
    "        recall_result = recall_metric.compute(\n",
    "            predictions=predictions,\n",
    "            references=labels,\n",
    "            average=\"weighted\",\n",
    "            zero_division=0\n",
    "        )\n",
    "        metrics[\"recall\"] = recall_result[\"recall\"]\n",
    "    \n",
    "        # F1 Score\n",
    "        f1_metric = evaluate.load(\"f1\")\n",
    "        f1_result = f1_metric.compute(\n",
    "            predictions=predictions,\n",
    "            references=labels,\n",
    "            average=\"weighted\"\n",
    "        )\n",
    "        metrics[\"f1\"] = f1_result[\"f1\"]\n",
    "    \n",
    "        return metrics\n",
    "\n",
    "    def train_model(self, dds: DatasetDict) -> Trainer:\n",
    "        \"\"\"Initialize and train the model\"\"\"\n",
    "        try:\n",
    "            # Initialize model and data collator\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                CFG.model_name,\n",
    "                num_labels=2\n",
    "            )\n",
    "            data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "\n",
    "            # Set up training arguments\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=CFG.output_dir,\n",
    "                learning_rate=CFG.learning_rate,\n",
    "                per_device_train_batch_size=CFG.batch_size,\n",
    "                per_device_eval_batch_size=CFG.batch_size,\n",
    "                num_train_epochs=CFG.epochs,\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                save_strategy=\"epoch\",\n",
    "                save_total_limit=2,\n",
    "                fp16=torch.cuda.is_available(),\n",
    "                logging_dir=f\"{CFG.output_dir}/logs\",\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"f1\",\n",
    "            )\n",
    "\n",
    "            # Initialize trainer\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=dds['train'],\n",
    "                eval_dataset=dds['test'],\n",
    "                tokenizer=self.tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=self.compute_metrics,\n",
    "            )\n",
    "\n",
    "            # Train the model\n",
    "            trainer.train()\n",
    "            return trainer\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in model training: {e}\")\n",
    "\n",
    "    def get_predictions(self, trainer: Trainer, eval_dataset: Dataset) -> pd.DataFrame:\n",
    "        \"\"\"Get probability predictions for the evaluation dataset and return as DataFrame\"\"\"\n",
    "        try:\n",
    "            predictions = trainer.predict(eval_dataset)\n",
    "            logits = predictions.predictions\n",
    "            probabilities = softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "\n",
    "            # Create DataFrame with model-specific column names\n",
    "            model_name = CFG.model_name.split('/')[-1]  # Get last part of model name\n",
    "            df_predictions = pd.DataFrame(\n",
    "                probabilities,\n",
    "                columns=[f'p0_{model_name}', f'p1_{model_name}']\n",
    "            )\n",
    "\n",
    "            return df_predictions\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in getting predictions: {e}\")\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up GPU memory\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        # Initialize trainer\n",
    "        text_classifier = TextClassificationTrainer()\n",
    "\n",
    "        # Load and preprocess data\n",
    "        logger.info(\"Loading data...\")\n",
    "        train, test = text_classifier.load_data()\n",
    "\n",
    "        logger.info(\"Preprocessing data...\")\n",
    "        dds, eval_dataset = text_classifier.preprocess_data(train, test)\n",
    "\n",
    "        # Train model\n",
    "        logger.info(\"Training model...\")\n",
    "        trainer = text_classifier.train_model(dds)\n",
    "\n",
    "        # Get predictions as DataFrame\n",
    "        logger.info(\"Getting predictions...\")\n",
    "        df_predictions = text_classifier.get_predictions(trainer, eval_dataset)\n",
    "\n",
    "        # Save predictions DataFrame\n",
    "        output_path = f\"{CFG.output_dir}/predictions_{CFG.model_name.split('/')[-1]}.csv\"\n",
    "        df_predictions.to_csv(output_path, index=False)\n",
    "        logger.info(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "        # Cleanup\n",
    "        text_classifier.cleanup()\n",
    "\n",
    "        return df_predictions\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa7a70-6afe-47e8-8759-e618097927b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "606f1cc0-4fef-43d2-81a0-48165c228deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf49254-2fff-417b-b44f-c45b85cdf7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
